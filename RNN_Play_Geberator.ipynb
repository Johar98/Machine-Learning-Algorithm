{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN_Play_Geberator.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPcLDsKzAlXeRxNVVjK6JoE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Johar98/Machine-Learning-Algorithm/blob/main/RNN_Play_Geberator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sQl8J5Zj5X9"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "from keras.preprocessing import sequence\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "684nQsnGknSb",
        "outputId": "b36ca695-d8f8-4cdc-963e-b77eb25895eb"
      },
      "source": [
        "path_to_file = tf.keras.utils.get_file(\"shakespears.txt\",\n",
        "                                       \"https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJ94kImll3yp",
        "outputId": "4abdfb81-5031-4263-bdb2-dbf8d3b5d638"
      },
      "source": [
        "#read, then decode from py2 compat\n",
        "text = open(path_to_file , 'rb').read().decode(encoding = 'utf-8')\n",
        "#length of text is the number of character in it\n",
        "print('length of text : {} character'.format(len(text)))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "length of text : 1115394 character\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "phb5xH_Gm1tm",
        "outputId": "fd4386e3-d6b1-4c97-ab3d-c14d9f195a28"
      },
      "source": [
        "#take a look at the first 250 character in text\n",
        "print(text[:250])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVtiQX68nIsK"
      },
      "source": [
        "vocab = sorted(set(text)) #sort all the unique character in a sat\n",
        "#creating a mapping from unique character to indices\n",
        "char2idx = {u:i for i,u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "def text_to_int(text):\n",
        "  return np.array([char2idx[c] for c in text])\n",
        "text_as_int = text_to_int(text)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jXvEgsxgoINC",
        "outputId": "6e611360-bdcd-4ea4-ac68-b2107b66c18b"
      },
      "source": [
        "#lets look at how part of our text in encoded\n",
        "print(\"text:\" , text[:13])\n",
        "print(\"encoded:\" , text_to_int(text[:13]))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "text: First Citizen\n",
            "encoded: [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S58J-uaIodFi",
        "outputId": "7f97338c-e17b-4403-ee7e-9aa185e28fc9"
      },
      "source": [
        "def int_to_text(ints):\n",
        "  try:\n",
        "    ints = ints.numpy()\n",
        "  except:\n",
        "      pass\n",
        "  return \"\".join(idx2char[ints])\n",
        "print(int_to_text(text_as_int[:13]))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First Citizen\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCLih1XTpNyJ"
      },
      "source": [
        "seq_length = 100 #length of sequence for a training example\n",
        "exmples_per_epoch = len(text)//(seq_length+1)\n",
        "#create training example/targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmzoGiqJp84f"
      },
      "source": [
        "sequences = char_dataset.batch(seq_length+1 , drop_remainder = True)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WEC-caIqNvT"
      },
      "source": [
        "def split_input_target(chunk): #for the example :hello\n",
        "  input_text = chunk[:-1] #hell\n",
        "  target_text = chunk[1:] #ello\n",
        "  return input_text , target_text #hell , ello\n",
        "dataset = sequences.map(split_input_target) # we can map to apply the above function to every entry"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-qtN97NrH0q",
        "outputId": "78b7a8d8-9595-43f9-f451-0a50d76b3e5c"
      },
      "source": [
        "for x,y in dataset.take(2):\n",
        "  print(\"\\n\\nEXAMPLE\\n\")\n",
        "  print(\"INPUT\")\n",
        "  print(int_to_text(x))\n",
        "  print(\"\\nOUTPUT\")\n",
        "  print(int_to_text(y))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "EXAMPLE\n",
            "\n",
            "INPUT\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n",
            "\n",
            "OUTPUT\n",
            "irst Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You \n",
            "\n",
            "\n",
            "EXAMPLE\n",
            "\n",
            "INPUT\n",
            "are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you \n",
            "\n",
            "OUTPUT\n",
            "re all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you k\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1y5eIBjsPNd"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "VOCAB_SIZE = len(vocab)\n",
        "EMBEDDING_DIM = 256\n",
        "RNN_UNITS = 1024\n",
        "#buffer size to shuffle the dataset\n",
        "#(tf data is designed to work with possiby infinite sequence so it doesn't attempt to shuffle the entire sequence in memory , instead\n",
        "# it maintains a buffer in which it shuffle elements)\n",
        "BUFFER_SIZE = 10000\n",
        "data = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE , drop_remainder = True)\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lbSKzQj2slm6",
        "outputId": "7fdbf9d1-9f50-43ca-8a6d-61ede60592e9"
      },
      "source": [
        "def build_model(vocab_size , embedding_dim , rnn_units , batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "                               tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape = [batch_size , None]),\n",
        "                               tf.keras.layers.LSTM(rnn_units, return_sequences=True, stateful = True, recurrent_initializer=\"glorot_uniform\"),\n",
        "                               tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model\n",
        "model = build_model(VOCAB_SIZE , EMBEDDING_DIM , RNN_UNITS , BATCH_SIZE)\n",
        "model.summary"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method Model.summary of <tensorflow.python.keras.engine.sequential.Sequential object at 0x7fcc8d645390>>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qbmy8eZKdeps",
        "outputId": "c9c03342-8738-4fd3-a9d6-9ab8aadbfe7e"
      },
      "source": [
        "for input_example_batch , target_example_batch in data.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)\n",
        "  #ask our model for a prediction on our first batch of training data\n",
        "  print(example_batch_predictions.shape , \"#(batch_size , sequence_length , vocab_size)\")\n",
        "  #print out the output shape"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100, 65) #(batch_size , sequence_length , vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ew6rKvK-e0wY",
        "outputId": "88385416-f954-4acd-addd-997203a4395f"
      },
      "source": [
        "#we can see that the prediction is an array of 64 arrays , one for each entry in the batch\n",
        "print(len(example_batch_predictions))\n",
        "print(example_batch_predictions)\n",
        "#let's exam one prediction\n",
        "pred = example_batch_predictions[0]\n",
        "print(len(pred))\n",
        "print(pred) #notice this is a 2d array of length 100 , where each integerior array is the prediction for the next character at each time step\n",
        "#and finally well look at a predictions at the first timstep\n",
        "time_pred = pred[0]\n",
        "print(len(time_pred))\n",
        "print(time_pred)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "64\n",
            "tf.Tensor(\n",
            "[[[-6.62135892e-03 -4.87826439e-03 -2.44118576e-03 ...  4.56168829e-03\n",
            "    2.72521051e-04 -2.05360446e-03]\n",
            "  [-2.53627053e-03 -4.53067804e-03 -2.69192457e-03 ...  4.34534438e-03\n",
            "   -6.92740665e-04 -1.98881282e-03]\n",
            "  [-9.40695126e-03  3.63098038e-03 -6.82335673e-03 ...  6.83177495e-03\n",
            "   -4.51169861e-03  6.50289934e-03]\n",
            "  ...\n",
            "  [-5.91002079e-03  5.66960266e-03  5.67942159e-04 ... -4.21425560e-03\n",
            "   -1.23961875e-02 -5.94716286e-03]\n",
            "  [-1.17876558e-02  1.04077691e-02 -3.53369419e-03 ...  1.89227052e-04\n",
            "   -1.28982514e-02  1.87355373e-03]\n",
            "  [-1.66081395e-02  1.41211795e-02 -6.70389552e-03 ...  3.99772683e-03\n",
            "   -1.28646214e-02  7.73449335e-03]]\n",
            "\n",
            " [[-4.10033669e-03 -5.49616176e-04  4.88710171e-03 ... -5.77423256e-03\n",
            "   -2.91904691e-03 -7.63809029e-03]\n",
            "  [-1.59133971e-03 -2.90562119e-03  7.07872072e-03 ...  1.32895657e-05\n",
            "    2.26457167e-04 -6.30024541e-03]\n",
            "  [ 9.39406455e-05 -2.63957540e-04  4.23443969e-03 ... -3.18317930e-03\n",
            "   -3.03395186e-03 -2.61153700e-03]\n",
            "  ...\n",
            "  [-2.79926392e-03 -1.97212212e-03  1.24855209e-02 ... -3.13867419e-03\n",
            "    4.90949163e-03 -1.11586624e-03]\n",
            "  [-6.75167236e-03  4.14271932e-03  1.13908490e-02 ... -7.35135190e-03\n",
            "    6.19067438e-03 -1.42433727e-03]\n",
            "  [-3.28365527e-03  2.50785891e-03  8.48584156e-03 ... -1.46641815e-03\n",
            "    4.93009482e-03  2.66916631e-03]]\n",
            "\n",
            " [[-9.43025108e-04 -5.05037140e-03 -8.28098797e-04 ... -8.29738565e-05\n",
            "   -1.38944236e-03  1.27757422e-03]\n",
            "  [-9.08343820e-04 -8.57359916e-03 -1.95240241e-03 ... -1.58444978e-04\n",
            "   -3.57960491e-03  2.40859250e-03]\n",
            "  [-1.93540682e-03 -6.60646195e-03  7.56146386e-03 ... -1.64427771e-03\n",
            "   -5.67569770e-03  3.78831616e-03]\n",
            "  ...\n",
            "  [-5.30487206e-03  1.50632430e-02 -2.63555790e-03 ...  9.36746970e-03\n",
            "   -4.97356290e-04  2.13371124e-03]\n",
            "  [-4.96679544e-03  9.77039803e-03  7.46220769e-03 ...  5.35489153e-03\n",
            "   -8.99733743e-04  1.53153436e-03]\n",
            "  [-4.18905169e-04  5.75492531e-03  1.96190318e-04 ...  5.56921586e-03\n",
            "   -5.87022863e-04 -3.71380895e-03]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-2.43764278e-03  3.03509505e-03  1.19090080e-03 ...  7.77317956e-03\n",
            "   -1.09304779e-03 -1.34921831e-03]\n",
            "  [-3.26836528e-03  4.74685524e-03  2.29488686e-03 ...  1.32649858e-02\n",
            "   -1.29239389e-03 -2.93617719e-03]\n",
            "  [-3.36047006e-03  2.10615899e-03  1.18394932e-02 ...  7.95225240e-03\n",
            "   -8.86967871e-04 -1.63351488e-03]\n",
            "  ...\n",
            "  [-5.03377523e-03  5.70453471e-04 -6.07135659e-03 ... -1.49865751e-03\n",
            "    8.55831057e-03  8.70965770e-04]\n",
            "  [-2.36022496e-03  6.66937907e-04 -4.65872791e-03 ...  2.90259812e-03\n",
            "    6.45461958e-03  6.45779539e-03]\n",
            "  [ 7.96581153e-05  3.37319239e-03 -6.83204457e-03 ... -2.11762567e-03\n",
            "    2.54757912e-03  1.04023619e-02]]\n",
            "\n",
            " [[ 1.22959912e-03 -1.18135754e-03  1.28368312e-03 ...  2.77294964e-03\n",
            "   -1.33935153e-03  3.60128260e-03]\n",
            "  [ 2.53188261e-03  5.16467728e-04  6.77813124e-03 ...  5.71407145e-04\n",
            "   -8.52061342e-03  7.20982254e-03]\n",
            "  [-6.10521762e-04 -7.96866952e-04  1.51257738e-02 ... -1.25154504e-03\n",
            "   -7.50330277e-03  6.98519172e-03]\n",
            "  ...\n",
            "  [-4.30310750e-03  1.79357640e-03  6.00970862e-03 ...  7.50599802e-03\n",
            "   -6.46668300e-03  3.20913317e-03]\n",
            "  [-1.66689930e-03  1.75901921e-04  3.62411560e-03 ...  2.62061041e-03\n",
            "   -4.42668376e-03  6.17790269e-04]\n",
            "  [-2.76504806e-03  2.84660980e-03  2.35925033e-03 ...  9.74880252e-03\n",
            "   -3.79185216e-03 -5.22996066e-04]]\n",
            "\n",
            " [[-2.43764278e-03  3.03509505e-03  1.19090080e-03 ...  7.77317956e-03\n",
            "   -1.09304779e-03 -1.34921831e-03]\n",
            "  [ 3.98314267e-04  5.23768133e-04  2.37856945e-03 ...  8.24035797e-03\n",
            "   -1.53524475e-03  1.97642529e-03]\n",
            "  [ 3.69317620e-03 -1.32653466e-03 -1.88328233e-03 ...  6.52573351e-03\n",
            "   -1.40913669e-03 -2.03694333e-03]\n",
            "  ...\n",
            "  [-3.58510716e-03  2.57049664e-03  1.82959754e-02 ...  4.25098510e-03\n",
            "   -3.15052248e-03  3.56385950e-03]\n",
            "  [-3.98060726e-03  2.24316563e-03  1.17406901e-02 ...  5.08991862e-03\n",
            "   -4.13190993e-03  3.72604071e-03]\n",
            "  [-1.43943774e-03  4.07281052e-03  1.40468264e-02 ...  3.37711279e-03\n",
            "   -1.05269672e-02  7.08429515e-03]]], shape=(64, 100, 65), dtype=float32)\n",
            "100\n",
            "tf.Tensor(\n",
            "[[-0.00662136 -0.00487826 -0.00244119 ...  0.00456169  0.00027252\n",
            "  -0.0020536 ]\n",
            " [-0.00253627 -0.00453068 -0.00269192 ...  0.00434534 -0.00069274\n",
            "  -0.00198881]\n",
            " [-0.00940695  0.00363098 -0.00682336 ...  0.00683177 -0.0045117\n",
            "   0.0065029 ]\n",
            " ...\n",
            " [-0.00591002  0.0056696   0.00056794 ... -0.00421426 -0.01239619\n",
            "  -0.00594716]\n",
            " [-0.01178766  0.01040777 -0.00353369 ...  0.00018923 -0.01289825\n",
            "   0.00187355]\n",
            " [-0.01660814  0.01412118 -0.0067039  ...  0.00399773 -0.01286462\n",
            "   0.00773449]], shape=(100, 65), dtype=float32)\n",
            "65\n",
            "tf.Tensor(\n",
            "[-6.6213589e-03 -4.8782644e-03 -2.4411858e-03 -4.4952780e-03\n",
            "  4.4226027e-03 -2.6292261e-03  2.4741585e-04  3.5956253e-03\n",
            "  4.0816334e-03 -1.4368043e-03 -2.7596240e-04  1.7699915e-03\n",
            "  3.1415820e-03  3.4261779e-03 -5.5152029e-03  1.9223858e-03\n",
            "  4.3554474e-03 -1.4948053e-03  6.8861060e-04 -3.0390641e-03\n",
            "  1.0812284e-03  1.1961996e-03  6.2608160e-03  2.0214347e-03\n",
            " -4.0410412e-04  3.2316297e-03  1.6039527e-03 -4.3743993e-03\n",
            " -2.8834017e-03 -4.8662107e-03 -1.2690439e-03  3.3651548e-04\n",
            "  3.6148783e-03 -1.5690777e-04 -4.2938972e-03 -6.3701137e-04\n",
            " -4.8593264e-03 -2.3214193e-03 -4.1277339e-03  4.4731644e-04\n",
            " -1.9901900e-03  2.3082220e-03 -2.0817993e-03  7.6093152e-04\n",
            "  7.8721147e-05  3.6637066e-03  2.7780994e-04 -4.4402299e-03\n",
            " -3.2207026e-04  5.4826667e-03  1.8209161e-03 -4.9975477e-03\n",
            "  2.4670956e-03 -3.0178977e-03 -6.3773926e-04 -3.2375669e-03\n",
            " -2.9242560e-03 -1.0934740e-03 -2.4361203e-03  7.8872620e-04\n",
            "  2.5419891e-03 -8.7906286e-04  4.5616883e-03  2.7252105e-04\n",
            " -2.0536045e-03], shape=(65,), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Yh-9OyJ6gmL-",
        "outputId": "8675abcf-06e7-40cc-e692-5b12ea0a30ee"
      },
      "source": [
        "#if we want to determine the predicted character we need to sample the output distribution(pick a value base on probabilities)\n",
        "sampled_indices = tf.random.categorical(pred , num_samples=1)\n",
        "#now we can reshape that array and convert all the integers to numbers to see the actual characters\n",
        "sampled_indices = np.reshape(sampled_indices , (1, -1))[0]\n",
        "predicted_chars = int_to_text(sampled_indices)\n",
        "predicted_chars #and this is what the model predicted for training sequence 1"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"X&vCoFmQHtLiSkW---s:kEy&S;wvrpp'aftOLA$3&ZkhiT?,n:jq';d.DwhyWffpTlU:tB,rVog3kf;rhywJSwgvM,Qk,eioY?XF\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqZtS6wCiGCT"
      },
      "source": [
        "def loss(labels , logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels,logits,from_logits=True)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySjSDI6eiaQz"
      },
      "source": [
        "model.compile(optimizer = \"adam\" , loss=loss)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5FgHkl-ij7c"
      },
      "source": [
        "#directory where the checkpoints will be saved\n",
        "checkpoint_dir = \"./training_checkpoints\" #whose we want to save\n",
        "#name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir , \"ckpt_{epoch}\")\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath = checkpoint_prefix , \n",
        "    save_weights_only =True\n",
        ")"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "id": "HOiEmT6TjvBj",
        "outputId": "d8e7dd34-882e-4807-b24c-cafa86c0e2c7"
      },
      "source": [
        "history  = model.fit(data,epochs=40,callbacks=[checkpoint_callback])"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-93faf2196e5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpoint_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYRl-YGFmcMI"
      },
      "source": [
        "model = build_model(VOCAB_SIZE , EMBEDDING_DIM , DNN_UNITS , batch_size = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFolFNcymwZG"
      },
      "source": [
        "model.load_weight(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "model.build(tf.TensorShape([1,None]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kz3pisXYnIt3"
      },
      "source": [
        "def generate_text(model , start_string):\n",
        "  #evaluation step (generating text using the learned model)\n",
        "  #number of character to generate\n",
        "  num_generate = 800\n",
        "  #converting our start string to numbers(vectorizing)\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_sval , 0)\n",
        "  #empty string to store our results\n",
        "  text_generate = []\n",
        "  #low temperature results in more predictable text\n",
        "  #higher temperature results in more surprising text\n",
        "  #experiment to find the best setting\n",
        "  temperature = 1.0\n",
        "  #here batch size == 1\n",
        "  model.result_states()\n",
        "  for i in range(unm_generate):\n",
        "    predictions = model(input_eval)\n",
        "    #remove the batch dimension\n",
        "    predictions = tf.sequeeze(predictions , 0)\n",
        "    #using a categorical distribution to predict the character returned by the model\n",
        "    predictions = predictions / temperature\n",
        "    predicted_id = tf.random.categorical(predictions , num_sample=1)[-1 , 0].numpy()\n",
        "    #we pass the predicted characters as the next input to the model\n",
        "    #along with the previous hidden state\n",
        "    input_eval = tf.expand_dims([predicted_id] , 0)\n",
        "    text_generated.append(idx2char[predicted_id])\n",
        "return(start_string+\"\".join(text_generated))\n",
        "inp = input(\"type a starting string:\")\n",
        "print(generate_text(model , inp))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}